{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_original = pd.read_csv('./pickles/data_pd.csv') #Original Data\n",
    "topics = pickle.load(file=open('./pickles/mallet_learnt_topics.pickle','rb')) #Words for each topic\n",
    "data_cleaned = pickle.load(file=open('./pickles/mallet_pruned.pickle', 'rb')) #Feature vector for each PaperID\n",
    "user_map = pickle.load(file = open('./pickles/user_mappings.pickle', 'rb')) #Author Names -> PaperIDs\n",
    "user_map_name = pickle.load(file = open('./pickles/auth_paper_title.pickle', 'rb')) #Author Names -> Paper Titles\n",
    "user_research = pickle.load(file = open('./pickles/user_research_data.pickle', 'rb')) #Feature vector for each UserID\n",
    "user_id, id_user = pickle.load(file = open('./pickles/user_id_maps.pickle', 'rb')) #Name -> UserID, UserID -> Name\n",
    "paper_auth_id = pickle.load(file = open('./pickles/paper_auth_id.pickle', 'rb')) #PaperID -> AuthorID\n",
    "centers, k_ass, k_ass_rev = pickle.load(file = open('./pickles/kmeans_save.pickle', 'rb')) #KMeans centres\n",
    "paper_auth_name = pickle.load(file = open('./pickles/paper_auth_name.pickle', 'rb')) #PaperID -> Author Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(file=open('./pickles/scholarly_id_paper_title.pickle', 'wb'), obj= data_original['title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(file=open('./pickles/scholarly_id_user_name.pickle', 'wb'), obj=id_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>authors</th>\n",
       "      <th>tags</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minimax deviation strategies for machine learn...</td>\n",
       "      <td>The article is devoted to the problem of small...</td>\n",
       "      <td>1500196508</td>\n",
       "      <td>['Michail Schlesinger', 'Evgeniy Vodolazskiy']</td>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>1707.04849v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mlbench: How Good Are Machine Learning Clouds ...</td>\n",
       "      <td>We conduct an empirical study of machine learn...</td>\n",
       "      <td>1501365558</td>\n",
       "      <td>['Hantian Zhang', 'Luyuan Zeng', 'Wentao Wu', ...</td>\n",
       "      <td>['cs.DC', 'cs.LG', 'stat.ML']</td>\n",
       "      <td>1707.09562v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Introduction to Machine Learning: Class Notes ...</td>\n",
       "      <td>Introduction to Machine learning covering Stat...</td>\n",
       "      <td>1240486857</td>\n",
       "      <td>['Amnon Shashua']</td>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>0904.3664v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AutoCompete: A Framework for Machine Learning ...</td>\n",
       "      <td>In this paper, we propose AutoCompete, a highl...</td>\n",
       "      <td>1436368059</td>\n",
       "      <td>['Abhishek Thakur', 'Artus Krohn-Grimberghe']</td>\n",
       "      <td>['stat.ML', 'cs.LG']</td>\n",
       "      <td>1507.02188v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Joint Training of Deep Boltzmann Machines</td>\n",
       "      <td>We introduce a new method for training deep Bo...</td>\n",
       "      <td>1355277567</td>\n",
       "      <td>['Ian Goodfellow', 'Aaron Courville', 'Yoshua ...</td>\n",
       "      <td>['stat.ML', 'cs.LG']</td>\n",
       "      <td>1212.2686v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  Minimax deviation strategies for machine learn...   \n",
       "1           1  mlbench: How Good Are Machine Learning Clouds ...   \n",
       "2           2  Introduction to Machine Learning: Class Notes ...   \n",
       "3           3  AutoCompete: A Framework for Machine Learning ...   \n",
       "4           4          Joint Training of Deep Boltzmann Machines   \n",
       "\n",
       "                                             summary        date  \\\n",
       "0  The article is devoted to the problem of small...  1500196508   \n",
       "1  We conduct an empirical study of machine learn...  1501365558   \n",
       "2  Introduction to Machine learning covering Stat...  1240486857   \n",
       "3  In this paper, we propose AutoCompete, a highl...  1436368059   \n",
       "4  We introduce a new method for training deep Bo...  1355277567   \n",
       "\n",
       "                                             authors  \\\n",
       "0     ['Michail Schlesinger', 'Evgeniy Vodolazskiy']   \n",
       "1  ['Hantian Zhang', 'Luyuan Zeng', 'Wentao Wu', ...   \n",
       "2                                  ['Amnon Shashua']   \n",
       "3      ['Abhishek Thakur', 'Artus Krohn-Grimberghe']   \n",
       "4  ['Ian Goodfellow', 'Aaron Courville', 'Yoshua ...   \n",
       "\n",
       "                            tags            id  \n",
       "0                      ['cs.LG']  1707.04849v1  \n",
       "1  ['cs.DC', 'cs.LG', 'stat.ML']  1707.09562v2  \n",
       "2                      ['cs.LG']   0904.3664v1  \n",
       "3           ['stat.ML', 'cs.LG']  1507.02188v1  \n",
       "4           ['stat.ML', 'cs.LG']   1212.2686v1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_name_id(name):\n",
    "    name = unidecode.unidecode(name)\n",
    "    surname = name[name.rfind(' ')+1:]\n",
    "    surname = surname.replace('-', '_')\n",
    "    first_name = name[0]\n",
    "    return surname + '_'+ first_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heinavaara_O'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_name_id('Otte Hein√§vaara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_names = list(user_id.keys())\n",
    "user_id_type = ['author' for name in user_id_names]\n",
    "user_id_ids = [user_id[name] for name in user_id_names]\n",
    "user_id_detail_id = [user_map[name] for name in user_id_names]\n",
    "user_id_detail_name = [user_map_name[name] for name in user_id_names]\n",
    "user_arxiv_id = [transform_name_id(name) for name in user_id_names]\n",
    "paper_titles = data_original['title'].values\n",
    "paper_arxiv_id = data_original['id'].values\n",
    "paper_type = ['paper' for paper in paper_titles]\n",
    "paper_id = list(range(len(paper_titles)))\n",
    "paper_id_detail_id = paper_auth_id\n",
    "paper_id_detail_name = paper_auth_name\n",
    "paper_date = data_original['date'].values\n",
    "col_names = np.array(list(user_id_names) + list(paper_titles)).reshape(-1, 1)\n",
    "col_ids = np.array(user_id_ids + paper_id).reshape(-1, 1)\n",
    "col_type = np.array(user_id_type + paper_type).reshape(-1, 1)\n",
    "col_detail_id = np.array(user_id_detail_id + paper_id_detail_id).reshape(-1, 1)\n",
    "col_detail_name = np.array(user_id_detail_name + paper_id_detail_name).reshape(-1, 1)\n",
    "col_date = np.array(list([-1]*len(user_id_names)) + list(paper_date)).reshape(-1, 1)\n",
    "col_arxiv = np.array(list(user_arxiv_id) + list(paper_arxiv_id)).reshape(-1,1)\n",
    "search_key = dict(zip(list(list(user_id_names) + list(paper_titles)), list(range(len(col_names)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_matrix = (len(user_id_names), np.squeeze(np.stack((col_names,col_type, col_ids, col_detail_id, col_detail_name, col_date, col_arxiv), axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(file = open('./pickles/search_array.pickle', 'wb'), obj=search_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "def autocomplete_search(term, matrix, search_indx, limit = 100):\n",
    "    op = {}\n",
    "    op_papers = []\n",
    "    term = term.strip()\n",
    "    term_min = 3\n",
    "    show_paper = True\n",
    "    show_author = True\n",
    "    filter_surname = False\n",
    "    sn_chk = \"?surname=\"\n",
    "    paper_chk = \"?paper\"\n",
    "    author_chk = \"?author\"\n",
    "    surname = \"\"\n",
    "    if paper_chk in term:\n",
    "        show_author = False\n",
    "        term = term.replace(paper_chk, '').strip()\n",
    "    if sn_chk in term and ';' in term:\n",
    "        surname = term.lower()[term.find(sn_chk)+len(sn_chk):term.find(';', term.find(sn_chk))].strip()\n",
    "        term = term.replace(term[term.find(sn_chk):term.find(';', term.find(sn_chk))+1], \"\").strip()\n",
    "        filter_surname = True\n",
    "    if author_chk in term:\n",
    "        show_paper = False\n",
    "        term = term.replace(author_chk, '').strip()\n",
    "    if len(term) < term_min and not filter_surname: \n",
    "        return {}\n",
    "    for idx, val in np.ndenumerate(matrix[:, search_indx]):\n",
    "        row = matrix[idx, :]\n",
    "        val = val.replace(\"\\n\", \"\").replace(\"  \", \" \")\n",
    "        if len(term) >= term_min and (term in val or term.lower() in val.lower()):\n",
    "            row[0][search_indx] = val.strip()\n",
    "            if row[0][1] == 'author' and show_author:\n",
    "                if filter_surname:\n",
    "                    if surname.lower() in val.split()[-1].lower():\n",
    "                        op[int(np.squeeze(idx))] = row\n",
    "                else:\n",
    "                    op[int(np.squeeze(idx))] = row\n",
    "            elif row[0][1] == 'paper' and show_paper:\n",
    "                if filter_surname:\n",
    "                    for auth in row[0][4]:\n",
    "                        if surname.lower() in auth.split()[-1].lower():\n",
    "                            op_papers.append((int(np.squeeze(idx)), row))\n",
    "                            break\n",
    "                else:\n",
    "                    op_papers.append((int(np.squeeze(idx)), row))\n",
    "        elif row[0][1] == 'paper' and filter_surname and len(term.strip()) == 0:\n",
    "                for auth in row[0][4]:\n",
    "                    if surname in auth.split()[-1].lower():\n",
    "                        op_papers.append((int(np.squeeze(idx)), row))\n",
    "                        break\n",
    "        if len(op.keys()) + len(op_papers) > limit:\n",
    "            break\n",
    "    if len(op_papers) + len(op.keys()) == 0:\n",
    "        return []\n",
    "    results = {\"results\":{\"category1\":{\"name\":\"Authors\", \"results\":[]}, \"category2\":{\"name\": \"Papers\", \"results\":[]}}}\n",
    "    for op_key in op:\n",
    "        in_data = op[op_key][0]\n",
    "        desc = in_data[4]\n",
    "        title = in_data[0]\n",
    "        url = \"\"\n",
    "        if in_data[1] == 'author':\n",
    "            title = title.title()\n",
    "            desc = str(len(in_data[3])) + \" paper(s) including: '\" + random.choice(in_data[4]).replace(\"\\n\", \"\").replace(\"  \", \" \").strip() + \"'\"\n",
    "            url = \"/author/\" + str(in_data[2])\n",
    "            entry = {\"title\":title, \"description\":desc, \"url\":url}\n",
    "            results['results']['category1']['results'].append(entry)\n",
    "        else: \n",
    "            pass\n",
    "    #Sort list of papers\n",
    "    op_papers = sorted(op_papers, key=lambda k: k[1][0][5], reverse= True)\n",
    "    for paper in op_papers:\n",
    "        in_data = paper[1][0]\n",
    "        desc = in_data[4]\n",
    "        title = in_data[0].replace(\"\\n\", \"\").replace(\"  \", \" \")\n",
    "        if in_data[1] == 'paper':\n",
    "            months = [\"Unknown\", \"January\", \"Febuary\", \"March\", \"April\", \"May\",\"June\",\n",
    "                      \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "            date = datetime.datetime.utcfromtimestamp(in_data[5])\n",
    "            desc = ', '.join(desc)\n",
    "            desc = '(' + months[date.month] + ' '  + str(date.year) +') ' + desc\n",
    "            url = \"/paper/\" + str(in_data[2])\n",
    "            if len(desc) > 70:\n",
    "                desc = desc[:desc.find(', ',40)] + \" and others\"\n",
    "            entry = {\"title\":title, \"description\":desc, \"url\":url}\n",
    "            results['results']['category2']['results'].append(entry)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-f43e13c6848f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-f43e13c6848f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Ensemble of Generative and Discriminative Techniques for Sentiment  Analysis of Movie Reviews\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Ensemble of Generative and Discriminative Techniques for Sentiment  Analysis of Movie Reviews\n",
    "Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews\n",
    "Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'category1': {'name': 'Authors', 'results': []},\n",
       "  'category2': {'name': 'Papers',\n",
       "   'results': [{'description': '(December 2014) Gr√©goire Mesnil, Tomas Mikolov and others',\n",
       "     'title': 'Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews',\n",
       "     'url': '/paper/23709'}]}}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocomplete_search('Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews', search_matrix[1], 0, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
